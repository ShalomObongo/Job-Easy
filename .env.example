# Job-Easy Configuration
# Copy this file to .env and customize for your environment

# =============================================================================
# OPERATING MODE
# =============================================================================

# Mode: 'single' for processing one job URL, 'autonomous' for batch processing
MODE=single

# =============================================================================
# SAFETY SETTINGS
# =============================================================================

# Auto-submit applications without confirmation (NOT RECOMMENDED)
# Default: false - always require explicit confirmation before submitting
AUTO_SUBMIT=false

# Maximum applications to process per day (autonomous mode only)
MAX_APPLICATIONS_PER_DAY=10

# =============================================================================
# PATHS
# =============================================================================

# Path to the SQLite tracker database
TRACKER_DB_PATH=./data/tracker.db

# Directory for generated artifacts (resumes, cover letters, proofs)
OUTPUT_DIR=./artifacts

# =============================================================================
# RUNNER SETTINGS (Application Form Automation)
# =============================================================================

# Blocklist-first: only these domains are blocked; everything else is allowed.
# Supports patterns like: example.com, *.example.com, http*://example.com
# Comma-separated or JSON list.
# PROHIBITED_DOMAINS=example.com, *.evil.com

# Append-only log of encountered (non-prohibited) domains
ALLOWLIST_LOG_PATH=./data/allowlist.log

# Persistent Q&A bank for application questions
QA_BANK_PATH=./data/qa_bank.json

# Runner browser behavior
RUNNER_HEADLESS=false
RUNNER_WINDOW_WIDTH=1280
RUNNER_WINDOW_HEIGHT=720

# Runner agent behavior
RUNNER_MAX_FAILURES=3
RUNNER_MAX_ACTIONS_PER_STEP=4
RUNNER_STEP_TIMEOUT=120
RUNNER_USE_VISION=auto

# Automatically answer yes to non-submit prompts (fit skip/review, document approval).
# This does NOT bypass the final submit gate (still requires typing YES).
RUNNER_ASSUME_YES=false

# Runner YOLO mode (best-effort auto-answering for application questions)
# Default: false (opt-in)
RUNNER_YOLO_MODE=false

# Runner LLM override (optional; defaults to EXTRACTOR_LLM_* if unset)
# RUNNER_LLM_PROVIDER=auto
# RUNNER_LLM_API_KEY=your-api-key-here
# RUNNER_LLM_BASE_URL=http://localhost:1234/v1
# RUNNER_LLM_MODEL=gpt-4o
# RUNNER_LLM_REASONING_EFFORT=low

# =============================================================================
# CHROME PROFILE (Optional)
# =============================================================================

# Use an existing Chrome profile to reuse logged-in sessions
# WARNING: Make a copy of your profile before using automation to avoid data corruption
USE_EXISTING_CHROME_PROFILE=false

# Chrome user data directory (required if using existing profile)
# macOS example (your system):
CHROME_USER_DATA_DIR=/Users/shalom/Library/Application Support/Google/Chrome
# Windows: C:\Users\<username>\AppData\Local\Google\Chrome\User Data
# Linux: /home/<username>/.config/google-chrome

# Chrome profile directory name
# Available profiles on this system: Default, Profile 1, Profile 2, LinkedIn Automation
CHROME_PROFILE_DIR=Default
#
# Note: If you see "Permission denied" errors when Browser Use tries to copy your profile,
# your selected profile likely contains unreadable (often root-owned) files. In that case,
# choose a different profile folder (e.g. `Profile 1`) or fix the file ownership/permissions.

# Chrome profile mode:
# - auto: try copy first, fall back to direct on permission errors
# - copy: safest (Browser Use copies your profile to a temp dir)
# - direct: use your profile in place (riskier; close Chrome first)
CHROME_PROFILE_MODE=auto

# =============================================================================
# LLM API (Required for tailoring)
# =============================================================================

# Configure LLM credentials using the module-specific prefixes:
# - EXTRACTOR_LLM_* (Extractor + Runner default)
# - TAILORING_LLM_* (Tailoring; falls back to EXTRACTOR_LLM_* if unset)
# - RUNNER_LLM_* (Runner override; falls back to EXTRACTOR_LLM_* if unset)
#
# You can also set provider-native keys like OPENAI_API_KEY / ANTHROPIC_API_KEY.
# (The code still supports LLM_API_KEY as a fallback for OpenAI/Anthropic, but prefer the prefixes above.)
# LLM_API_KEY=your-api-key-here

# =============================================================================
# JOB EXTRACTOR SETTINGS
# =============================================================================

# LLM Provider: 'openai', 'anthropic', 'browser_use', or 'auto'
# 'auto' will try providers in order based on available API keys
EXTRACTOR_LLM_PROVIDER=auto

# API key for the extractor LLM (overrides LLM_API_KEY for extraction)
# EXTRACTOR_LLM_API_KEY=your-api-key-here

# Base URL for OpenAI-compatible endpoints (e.g., Azure OpenAI, local models)
# Leave empty to use the default provider URL
# Examples:
#   Azure OpenAI: https://your-resource.openai.azure.com/
#   Local (Ollama): http://localhost:11434/v1
#   Local (LM Studio): http://localhost:1234/v1
# EXTRACTOR_LLM_BASE_URL=

# Model ID for extraction (defaults: gpt-4o for OpenAI, claude-sonnet-4-20250514 for Anthropic)
# Examples: gpt-4o, gpt-4o-mini, claude-sonnet-4-20250514, llama3.2
# EXTRACTOR_LLM_MODEL=gpt-4o
#
# Reasoning effort for supported models (optional)
# Examples: none, minimal, low, medium, high, xhigh
# EXTRACTOR_LLM_REASONING_EFFORT=low

# Run browser in headless mode (no visible window)
EXTRACTOR_HEADLESS=true

# Timeout per step in seconds
EXTRACTOR_STEP_TIMEOUT=60

# Maximum retry attempts for failed steps
EXTRACTOR_MAX_FAILURES=3

# Vision mode: 'auto', 'true', or 'false'
# 'auto' enables vision when helpful for extraction
EXTRACTOR_USE_VISION=auto

# Browser Use creates a temp `user_data_dir` under macOS/Linux temp folders.
# Set this to true if you want to keep those directories for debugging (not recommended).
# EXTRACTOR_KEEP_BROWSER_USE_TEMP_DIRS=false
#
# If you already have many old temp dirs, run:
#   python scripts/cleanup_browser_use_temp_dirs.py --older-than-hours 24

# =============================================================================
# FIT SCORING SETTINGS
# =============================================================================

# Path to your user profile (YAML or JSON)
SCORING_PROFILE_PATH=profiles/profile.yaml

# Recommendation threshold and review margin
SCORING_FIT_SCORE_THRESHOLD=0.75
SCORING_REVIEW_MARGIN=0.05

# Scoring weights (must sum to 1.0)
SCORING_WEIGHT_MUST_HAVE=0.40
SCORING_WEIGHT_PREFERRED=0.20
SCORING_WEIGHT_EXPERIENCE=0.25
SCORING_WEIGHT_EDUCATION=0.15

# Skill matching settings
SCORING_SKILL_FUZZY_MATCH=true
SCORING_SKILL_FUZZY_THRESHOLD=0.85
SCORING_EXPERIENCE_TOLERANCE_YEARS=2

# Constraint modes (true = hard skip, false = soft warning)
SCORING_LOCATION_STRICT=false
SCORING_VISA_STRICT=true
SCORING_SALARY_STRICT=false

# Scoring mode
# - deterministic: weighted scoring only
# - llm: LLM-primary scoring (constraints still deterministic; falls back on failure)
SCORING_SCORING_MODE=deterministic

# LLM scoring settings (used when SCORING_SCORING_MODE=llm)
# LLM Provider: openai, anthropic, etc.
# SCORING_LLM_PROVIDER=openai
# SCORING_LLM_MODEL=gpt-4o
# SCORING_LLM_API_KEY=your-api-key-here
# Base URL for OpenAI-compatible endpoints (e.g., Azure OpenAI, local models)
# SCORING_LLM_BASE_URL=http://localhost:1234/v1
# Timeout in seconds for scoring LLM calls
# SCORING_LLM_TIMEOUT=60
# Maximum retry attempts for scoring LLM calls
# SCORING_LLM_MAX_RETRIES=1
# Reasoning effort for supported models (optional)
# Examples: disable, none, minimal, low, medium, high, xhigh
# SCORING_LLM_REASONING_EFFORT=low

# =============================================================================
# TAILORING SETTINGS (Resume & Cover Letter Generation)
# =============================================================================

# NOTE: Tailoring settings automatically fall back to EXTRACTOR_LLM_* values
# if not explicitly set. You only need to configure these if you want to use
# a different LLM for tailoring than for extraction.

# LLM Provider for tailoring (falls back to EXTRACTOR_LLM_PROVIDER)
# TAILORING_LLM_PROVIDER=openai

# LLM Model for tailoring (falls back to EXTRACTOR_LLM_MODEL)
# TAILORING_LLM_MODEL=gpt-4o
#
# Reasoning effort for supported models (falls back to EXTRACTOR_LLM_REASONING_EFFORT)
# Examples: disable, none, minimal, low, medium, high, xhigh
# TAILORING_LLM_REASONING_EFFORT=low

# API key for tailoring LLM (falls back to EXTRACTOR_LLM_API_KEY)
# TAILORING_LLM_API_KEY=your-api-key-here

# Base URL for tailoring LLM (falls back to EXTRACTOR_LLM_BASE_URL)
# Useful for local models or Azure OpenAI
# TAILORING_LLM_BASE_URL=http://localhost:4141/v1

# Maximum retry attempts for LLM calls
# NOTE: If requests are slow but successful on your LLM server, prefer increasing
# TAILORING_LLM_TIMEOUT over increasing retries (retries can create duplicate requests).
TAILORING_LLM_MAX_RETRIES=1

# Timeout in seconds for LLM calls (local models often need >60s)
TAILORING_LLM_TIMEOUT=300

# Cover letter word count targets
TAILORING_COVER_LETTER_MIN_WORDS=300
TAILORING_COVER_LETTER_MAX_WORDS=400

# =============================================================================
# LOGGING
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
