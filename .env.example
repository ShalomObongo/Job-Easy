# Job-Easy Configuration
# Copy this file to .env and customize for your environment

# =============================================================================
# OPERATING MODE
# =============================================================================

# Mode: 'single' for processing one job URL, 'autonomous' for batch processing
MODE=single

# =============================================================================
# SAFETY SETTINGS
# =============================================================================

# Auto-submit applications without confirmation (NOT RECOMMENDED)
# Default: false - always require explicit confirmation before submitting
AUTO_SUBMIT=false

# Maximum applications to process per day (autonomous mode only)
MAX_APPLICATIONS_PER_DAY=10

# =============================================================================
# PATHS
# =============================================================================

# Path to the SQLite tracker database
TRACKER_DB_PATH=./data/tracker.db

# Directory for generated artifacts (resumes, cover letters, proofs)
OUTPUT_DIR=./artifacts

# =============================================================================
# CHROME PROFILE (Optional)
# =============================================================================

# Use an existing Chrome profile to reuse logged-in sessions
# WARNING: Make a copy of your profile before using automation to avoid data corruption
USE_EXISTING_CHROME_PROFILE=false

# Chrome user data directory (required if using existing profile)
# macOS example (your system):
CHROME_USER_DATA_DIR=/Users/shalom/Library/Application Support/Google/Chrome
# Windows: C:\Users\<username>\AppData\Local\Google\Chrome\User Data
# Linux: /home/<username>/.config/google-chrome

# Chrome profile directory name
# Available profiles on this system: Default, Profile 1, Profile 2, LinkedIn Automation
CHROME_PROFILE_DIR=Default
#
# Note: If you see "Permission denied" errors when Browser Use tries to copy your profile,
# your selected profile likely contains unreadable (often root-owned) files. In that case,
# choose a different profile folder (e.g. `Profile 1`) or fix the file ownership/permissions.

# Chrome profile mode:
# - auto: try copy first, fall back to direct on permission errors
# - copy: safest (Browser Use copies your profile to a temp dir)
# - direct: use your profile in place (riskier; close Chrome first)
CHROME_PROFILE_MODE=auto

# =============================================================================
# LLM API (Required for tailoring)
# =============================================================================

# API key for your LLM provider (OpenAI, Anthropic, etc.)
# LLM_API_KEY=your-api-key-here

# =============================================================================
# JOB EXTRACTOR SETTINGS
# =============================================================================

# LLM Provider: 'openai', 'anthropic', 'browser_use', or 'auto'
# 'auto' will try providers in order based on available API keys
EXTRACTOR_LLM_PROVIDER=auto

# API key for the extractor LLM (overrides LLM_API_KEY for extraction)
# EXTRACTOR_LLM_API_KEY=your-api-key-here

# Base URL for OpenAI-compatible endpoints (e.g., Azure OpenAI, local models)
# Leave empty to use the default provider URL
# Examples:
#   Azure OpenAI: https://your-resource.openai.azure.com/
#   Local (Ollama): http://localhost:11434/v1
#   Local (LM Studio): http://localhost:1234/v1
# EXTRACTOR_LLM_BASE_URL=

# Model ID for extraction (defaults: gpt-4o for OpenAI, claude-sonnet-4-20250514 for Anthropic)
# Examples: gpt-4o, gpt-4o-mini, claude-sonnet-4-20250514, llama3.2
# EXTRACTOR_LLM_MODEL=gpt-4o

# Run browser in headless mode (no visible window)
EXTRACTOR_HEADLESS=true

# Timeout per step in seconds
EXTRACTOR_STEP_TIMEOUT=60

# Maximum retry attempts for failed steps
EXTRACTOR_MAX_FAILURES=3

# Vision mode: 'auto', 'true', or 'false'
# 'auto' enables vision when helpful for extraction
EXTRACTOR_USE_VISION=auto

# Browser Use creates a temp `user_data_dir` under macOS/Linux temp folders.
# Set this to true if you want to keep those directories for debugging (not recommended).
# EXTRACTOR_KEEP_BROWSER_USE_TEMP_DIRS=false
#
# If you already have many old temp dirs, run:
#   python scripts/cleanup_browser_use_temp_dirs.py --older-than-hours 24

# =============================================================================
# FIT SCORING SETTINGS
# =============================================================================

# Path to your user profile (YAML or JSON)
SCORING_PROFILE_PATH=profiles/profile.yaml

# Recommendation threshold and review margin
SCORING_FIT_SCORE_THRESHOLD=0.75
SCORING_REVIEW_MARGIN=0.05

# Scoring weights (must sum to 1.0)
SCORING_WEIGHT_MUST_HAVE=0.40
SCORING_WEIGHT_PREFERRED=0.20
SCORING_WEIGHT_EXPERIENCE=0.25
SCORING_WEIGHT_EDUCATION=0.15

# Skill matching settings
SCORING_SKILL_FUZZY_MATCH=true
SCORING_SKILL_FUZZY_THRESHOLD=0.85
SCORING_EXPERIENCE_TOLERANCE_YEARS=2

# Constraint modes (true = hard skip, false = soft warning)
SCORING_LOCATION_STRICT=false
SCORING_VISA_STRICT=true
SCORING_SALARY_STRICT=false

# =============================================================================
# LOGGING
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
